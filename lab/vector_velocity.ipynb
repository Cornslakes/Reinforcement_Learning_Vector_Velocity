{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1 ended with reward: -74.9488902707035\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# from gym_vectorvelocity.utils import test_with_random_moves\n",
    "\n",
    "# test_with_random_moves()\n",
    "# test_with_random_moves(episodes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from gym_vectorvelocity.utils import play_as_human\n",
    "\n",
    "# play_as_human(sound_volume=0.0, save_volume=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check observation and action space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.6.0 (SDL 2.28.4, Python 3.10.11)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "from gym_vectorvelocity import VectorVelocityEnv\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "\n",
    "\n",
    "# env modifications if needed\n",
    "GAMEOVER_PENALTY = 75\n",
    "MISSED_COIN_PENALTY = 3\n",
    "\n",
    "DODGED_OBSTACLE_REWARD = 1\n",
    "COLLECTED_COIN_REWARD = 12\n",
    "\n",
    "def create_env():\n",
    "    env = VectorVelocityEnv()\n",
    "    env.coin_missed_penalty = MISSED_COIN_PENALTY\n",
    "    env.game_over_penalty = GAMEOVER_PENALTY\n",
    "    env.dodged_obstacle_reward = DODGED_OBSTACLE_REWARD\n",
    "    env.coin_reward = COLLECTED_COIN_REWARD\n",
    "    return env\n",
    "\n",
    "env = create_env()\n",
    "check_env(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Env and set up GPU training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gymnasium import make\n",
    "\n",
    "env = make('VectorVelocity-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action Space: Discrete(3)\n",
      "Observation Space: Dict('coin_dists': Box(-1.0, 1.0, (40,), float32), 'coins': Box(-1.0, 1.0, (40,), float32), 'collected_coins': Discrete(20001), 'lane_coins': MultiDiscrete([4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4]), 'lane_obstacles': MultiDiscrete([4 4 4 4 4 4 4 4 4]), 'obstacle_dists': Box(-1.0, 1.0, (18,), float32), 'obstacles': Box(-1.0, 1.0, (18,), float32), 'player_pos': Box(0.0, 1.0, (1,), float32), 'score': Discrete(120001), 'speed': Discrete(20))\n"
     ]
    }
   ],
   "source": [
    "# Print the action space\n",
    "print(\"Action Space:\", env.action_space)\n",
    "\n",
    "# Print the observation space\n",
    "print(\"Observation Space:\", env.observation_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test first model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 373      |\n",
      "|    ep_rew_mean     | -58.6    |\n",
      "| time/              |          |\n",
      "|    fps             | 60       |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 34       |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "Mean reward: -15.040545600000002 +/- 93.87096577533228\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "def create_monitored_env():\n",
    "    env = create_env()  # Create your environment\n",
    "    env = Monitor(env)  # Wrap it with the Monitor wrapper\n",
    "    return env\n",
    "\n",
    "env = DummyVecEnv([create_monitored_env])\n",
    "\n",
    "# Initialize the PPO agent\n",
    "model = PPO(\"MultiInputPolicy\", env, verbose=1, device=device)\n",
    "\n",
    "# Train the model\n",
    "model.learn(total_timesteps=100)\n",
    "\n",
    "# Evaluate the trained model\n",
    "mean_reward, std_reward = evaluate_policy(model, model.get_env(), n_eval_episodes=10)\n",
    "print(f\"Mean reward: {mean_reward} +/- {std_reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using StableBaselines for multiprocessing - Model PPO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### edited after https://stable-baselines3.readthedocs.io/en/master/guide/examples.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Logging to ./tensorboard_logs/ppo\\PPO_1\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 371      |\n",
      "|    ep_rew_mean     | -59.5    |\n",
      "| time/              |          |\n",
      "|    fps             | 715      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 34       |\n",
      "|    total_timesteps | 24576    |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 420         |\n",
      "|    ep_rew_mean          | -40.2       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 422         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 116         |\n",
      "|    total_timesteps      | 49152       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010871346 |\n",
      "|    clip_fraction        | 0.0353      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.09       |\n",
      "|    explained_variance   | -0.000371   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.71        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.00182    |\n",
      "|    value_loss           | 37.8        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 430         |\n",
      "|    ep_rew_mean          | -40.3       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 374         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 197         |\n",
      "|    total_timesteps      | 73728       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008384595 |\n",
      "|    clip_fraction        | 0.00769     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.08       |\n",
      "|    explained_variance   | 0.686       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.44        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.000358   |\n",
      "|    value_loss           | 20.7        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 438          |\n",
      "|    ep_rew_mean          | -33.4        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 353          |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 278          |\n",
      "|    total_timesteps      | 98304        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0051045213 |\n",
      "|    clip_fraction        | 0.00794      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.06        |\n",
      "|    explained_variance   | 0.802        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.44         |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -3.98e-05    |\n",
      "|    value_loss           | 14.8         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 416         |\n",
      "|    ep_rew_mean          | -48.4       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 339         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 362         |\n",
      "|    total_timesteps      | 122880      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005645693 |\n",
      "|    clip_fraction        | 0.0308      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.01       |\n",
      "|    explained_variance   | 0.898       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.31        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.00168    |\n",
      "|    value_loss           | 9.79        |\n",
      "-----------------------------------------\n",
      "Mean reward: -62.799592 +/- 13.947501326138994\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "import os\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "#---------------------------------------------------------------------------\n",
    "env_id = \"VectorVelocity-v0\"\n",
    "#---------------------------------------------------------------------------\n",
    "seed =  42\n",
    "total_timesteps = 1e5\n",
    "n_train_env = os.cpu_count()\n",
    "#---------------------------------------------------------------------------\n",
    "model_save_dir = \"../models/ppo\"\n",
    "os.makedirs(model_save_dir, exist_ok=True)\n",
    "tensorboard_log_dir = \"../tensorboard_logs/ppo\"\n",
    "os.makedirs(tensorboard_log_dir, exist_ok=True)\n",
    "#---------------------------------------------------------------------------\n",
    "\n",
    "train_env = make_vec_env(env_id, n_envs=n_train_env, seed=seed, vec_env_cls=DummyVecEnv)\n",
    "\n",
    "model = PPO(\"MultiInputPolicy\", train_env, verbose=1, device=device, tensorboard_log=tensorboard_log_dir)\n",
    "model.learn(total_timesteps=total_timesteps)\n",
    "\n",
    "mean_reward_ppo, std_reward_ppo = evaluate_policy(model, model.get_env(), n_eval_episodes=10)\n",
    "print(f\"Mean reward: {mean_reward_ppo} +/- {std_reward_ppo}\")\n",
    "\n",
    "model.save(f\"{model_save_dir}/ppo_vector_velocity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![DQN Trainings History](tensorboard_logs\\screenshots\\ppo_trainings_history.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using StableBaselines for multiprocessing - Model DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Logging to ./tensorboard_logs/dqn\\DQN_1\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 242      |\n",
      "|    ep_rew_mean      | -68.9    |\n",
      "|    exploration_rate | 0.71     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4        |\n",
      "|    fps              | 679      |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 3048     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 6.93e-05 |\n",
      "|    n_updates        | 61       |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 301      |\n",
      "|    ep_rew_mean      | -65.1    |\n",
      "|    exploration_rate | 0.543    |\n",
      "| time/               |          |\n",
      "|    episodes         | 8        |\n",
      "|    fps              | 688      |\n",
      "|    time_elapsed     | 6        |\n",
      "|    total_timesteps  | 4812     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000706 |\n",
      "|    n_updates        | 98       |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 331      |\n",
      "|    ep_rew_mean      | -63.1    |\n",
      "|    exploration_rate | 0.392    |\n",
      "| time/               |          |\n",
      "|    episodes         | 12       |\n",
      "|    fps              | 692      |\n",
      "|    time_elapsed     | 9        |\n",
      "|    total_timesteps  | 6396     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000132 |\n",
      "|    n_updates        | 131      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 363      |\n",
      "|    ep_rew_mean      | -60.3    |\n",
      "|    exploration_rate | 0.213    |\n",
      "| time/               |          |\n",
      "|    episodes         | 16       |\n",
      "|    fps              | 689      |\n",
      "|    time_elapsed     | 12       |\n",
      "|    total_timesteps  | 8280     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000233 |\n",
      "|    n_updates        | 170      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 355      |\n",
      "|    ep_rew_mean      | -61.2    |\n",
      "|    exploration_rate | 0.0572   |\n",
      "| time/               |          |\n",
      "|    episodes         | 20       |\n",
      "|    fps              | 689      |\n",
      "|    time_elapsed     | 14       |\n",
      "|    total_timesteps  | 9924     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 2.33     |\n",
      "|    n_updates        | 204      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 377      |\n",
      "|    ep_rew_mean      | -58.5    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 24       |\n",
      "|    fps              | 689      |\n",
      "|    time_elapsed     | 16       |\n",
      "|    total_timesteps  | 11460    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00029  |\n",
      "|    n_updates        | 236      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 384      |\n",
      "|    ep_rew_mean      | -58.1    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 28       |\n",
      "|    fps              | 687      |\n",
      "|    time_elapsed     | 17       |\n",
      "|    total_timesteps  | 12204    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000376 |\n",
      "|    n_updates        | 252      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 373      |\n",
      "|    ep_rew_mean      | -59.2    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 32       |\n",
      "|    fps              | 687      |\n",
      "|    time_elapsed     | 19       |\n",
      "|    total_timesteps  | 13380    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 3.92e-05 |\n",
      "|    n_updates        | 276      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 368      |\n",
      "|    ep_rew_mean      | -59.7    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 36       |\n",
      "|    fps              | 687      |\n",
      "|    time_elapsed     | 22       |\n",
      "|    total_timesteps  | 15264    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000133 |\n",
      "|    n_updates        | 315      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 363      |\n",
      "|    ep_rew_mean      | -60.2    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 40       |\n",
      "|    fps              | 686      |\n",
      "|    time_elapsed     | 24       |\n",
      "|    total_timesteps  | 16620    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 3.37e-05 |\n",
      "|    n_updates        | 344      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 359      |\n",
      "|    ep_rew_mean      | -60.6    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 44       |\n",
      "|    fps              | 686      |\n",
      "|    time_elapsed     | 26       |\n",
      "|    total_timesteps  | 18264    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000112 |\n",
      "|    n_updates        | 378      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 362      |\n",
      "|    ep_rew_mean      | -60.4    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 48       |\n",
      "|    fps              | 686      |\n",
      "|    time_elapsed     | 28       |\n",
      "|    total_timesteps  | 19788    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 3.71e-05 |\n",
      "|    n_updates        | 410      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 360      |\n",
      "|    ep_rew_mean      | -60.6    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 52       |\n",
      "|    fps              | 686      |\n",
      "|    time_elapsed     | 29       |\n",
      "|    total_timesteps  | 20028    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000966 |\n",
      "|    n_updates        | 415      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 361      |\n",
      "|    ep_rew_mean      | -60.5    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 56       |\n",
      "|    fps              | 686      |\n",
      "|    time_elapsed     | 33       |\n",
      "|    total_timesteps  | 22716    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 7.21e-05 |\n",
      "|    n_updates        | 471      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 355      |\n",
      "|    ep_rew_mean      | -61      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 60       |\n",
      "|    fps              | 686      |\n",
      "|    time_elapsed     | 33       |\n",
      "|    total_timesteps  | 23316    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 2.33     |\n",
      "|    n_updates        | 483      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 355      |\n",
      "|    ep_rew_mean      | -61.1    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 64       |\n",
      "|    fps              | 686      |\n",
      "|    time_elapsed     | 35       |\n",
      "|    total_timesteps  | 24060    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 9.19e-05 |\n",
      "|    n_updates        | 499      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 355      |\n",
      "|    ep_rew_mean      | -61.1    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 68       |\n",
      "|    fps              | 686      |\n",
      "|    time_elapsed     | 39       |\n",
      "|    total_timesteps  | 26892    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 7.28e-05 |\n",
      "|    n_updates        | 558      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 353      |\n",
      "|    ep_rew_mean      | -61.3    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 72       |\n",
      "|    fps              | 686      |\n",
      "|    time_elapsed     | 39       |\n",
      "|    total_timesteps  | 27324    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 4.56e-05 |\n",
      "|    n_updates        | 567      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 354      |\n",
      "|    ep_rew_mean      | -61.3    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 76       |\n",
      "|    fps              | 686      |\n",
      "|    time_elapsed     | 41       |\n",
      "|    total_timesteps  | 28488    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 9.27e-05 |\n",
      "|    n_updates        | 591      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 354      |\n",
      "|    ep_rew_mean      | -61.3    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 80       |\n",
      "|    fps              | 686      |\n",
      "|    time_elapsed     | 44       |\n",
      "|    total_timesteps  | 30456    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000462 |\n",
      "|    n_updates        | 632      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 353      |\n",
      "|    ep_rew_mean      | -61.4    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 84       |\n",
      "|    fps              | 686      |\n",
      "|    time_elapsed     | 47       |\n",
      "|    total_timesteps  | 32376    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000923 |\n",
      "|    n_updates        | 672      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 354      |\n",
      "|    ep_rew_mean      | -61.4    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 88       |\n",
      "|    fps              | 685      |\n",
      "|    time_elapsed     | 52       |\n",
      "|    total_timesteps  | 36072    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 7.51e-05 |\n",
      "|    n_updates        | 749      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 364      |\n",
      "|    ep_rew_mean      | -60.2    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 92       |\n",
      "|    fps              | 684      |\n",
      "|    time_elapsed     | 54       |\n",
      "|    total_timesteps  | 37416    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 2.34     |\n",
      "|    n_updates        | 777      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 365      |\n",
      "|    ep_rew_mean      | -60.2    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 96       |\n",
      "|    fps              | 684      |\n",
      "|    time_elapsed     | 58       |\n",
      "|    total_timesteps  | 39912    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 9.88e-05 |\n",
      "|    n_updates        | 829      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 379      |\n",
      "|    ep_rew_mean      | -58.1    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 100      |\n",
      "|    fps              | 684      |\n",
      "|    time_elapsed     | 59       |\n",
      "|    total_timesteps  | 40812    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00027  |\n",
      "|    n_updates        | 848      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 385      |\n",
      "|    ep_rew_mean      | -57.7    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 104      |\n",
      "|    fps              | 684      |\n",
      "|    time_elapsed     | 62       |\n",
      "|    total_timesteps  | 42696    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000247 |\n",
      "|    n_updates        | 887      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 396      |\n",
      "|    ep_rew_mean      | -55.8    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 108      |\n",
      "|    fps              | 684      |\n",
      "|    time_elapsed     | 67       |\n",
      "|    total_timesteps  | 46020    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00056  |\n",
      "|    n_updates        | 956      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 401      |\n",
      "|    ep_rew_mean      | -55.4    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 112      |\n",
      "|    fps              | 684      |\n",
      "|    time_elapsed     | 70       |\n",
      "|    total_timesteps  | 48360    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000324 |\n",
      "|    n_updates        | 1005     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 403      |\n",
      "|    ep_rew_mean      | -55.1    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 116      |\n",
      "|    fps              | 684      |\n",
      "|    time_elapsed     | 73       |\n",
      "|    total_timesteps  | 50412    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000945 |\n",
      "|    n_updates        | 1048     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 404      |\n",
      "|    ep_rew_mean      | -55      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 120      |\n",
      "|    fps              | 684      |\n",
      "|    time_elapsed     | 74       |\n",
      "|    total_timesteps  | 50760    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00101  |\n",
      "|    n_updates        | 1055     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 412      |\n",
      "|    ep_rew_mean      | -53.5    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 124      |\n",
      "|    fps              | 684      |\n",
      "|    time_elapsed     | 78       |\n",
      "|    total_timesteps  | 53616    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000978 |\n",
      "|    n_updates        | 1114     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 412      |\n",
      "|    ep_rew_mean      | -53.6    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 128      |\n",
      "|    fps              | 684      |\n",
      "|    time_elapsed     | 80       |\n",
      "|    total_timesteps  | 55392    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000951 |\n",
      "|    n_updates        | 1151     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 423      |\n",
      "|    ep_rew_mean      | -52.1    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 132      |\n",
      "|    fps              | 685      |\n",
      "|    time_elapsed     | 82       |\n",
      "|    total_timesteps  | 56784    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000302 |\n",
      "|    n_updates        | 1180     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 425      |\n",
      "|    ep_rew_mean      | -52      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 136      |\n",
      "|    fps              | 686      |\n",
      "|    time_elapsed     | 86       |\n",
      "|    total_timesteps  | 59124    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000269 |\n",
      "|    n_updates        | 1229     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 425      |\n",
      "|    ep_rew_mean      | -52      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 140      |\n",
      "|    fps              | 687      |\n",
      "|    time_elapsed     | 88       |\n",
      "|    total_timesteps  | 60492    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00258  |\n",
      "|    n_updates        | 1258     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 428      |\n",
      "|    ep_rew_mean      | -51.7    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 144      |\n",
      "|    fps              | 687      |\n",
      "|    time_elapsed     | 93       |\n",
      "|    total_timesteps  | 63996    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00128  |\n",
      "|    n_updates        | 1331     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 442      |\n",
      "|    ep_rew_mean      | -49.7    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 148      |\n",
      "|    fps              | 688      |\n",
      "|    time_elapsed     | 95       |\n",
      "|    total_timesteps  | 65568    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000111 |\n",
      "|    n_updates        | 1363     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 453      |\n",
      "|    ep_rew_mean      | -48.6    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 152      |\n",
      "|    fps              | 689      |\n",
      "|    time_elapsed     | 97       |\n",
      "|    total_timesteps  | 67224    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000532 |\n",
      "|    n_updates        | 1398     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 454      |\n",
      "|    ep_rew_mean      | -48.6    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 156      |\n",
      "|    fps              | 689      |\n",
      "|    time_elapsed     | 101      |\n",
      "|    total_timesteps  | 69804    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00118  |\n",
      "|    n_updates        | 1452     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 461      |\n",
      "|    ep_rew_mean      | -48.1    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 160      |\n",
      "|    fps              | 689      |\n",
      "|    time_elapsed     | 103      |\n",
      "|    total_timesteps  | 71424    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000473 |\n",
      "|    n_updates        | 1485     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 460      |\n",
      "|    ep_rew_mean      | -48      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 164      |\n",
      "|    fps              | 689      |\n",
      "|    time_elapsed     | 107      |\n",
      "|    total_timesteps  | 73968    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000515 |\n",
      "|    n_updates        | 1538     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 465      |\n",
      "|    ep_rew_mean      | -47.7    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 168      |\n",
      "|    fps              | 689      |\n",
      "|    time_elapsed     | 110      |\n",
      "|    total_timesteps  | 76440    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000263 |\n",
      "|    n_updates        | 1590     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 471      |\n",
      "|    ep_rew_mean      | -47.2    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 172      |\n",
      "|    fps              | 690      |\n",
      "|    time_elapsed     | 115      |\n",
      "|    total_timesteps  | 79860    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 2.35     |\n",
      "|    n_updates        | 1661     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 491      |\n",
      "|    ep_rew_mean      | -42.2    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 176      |\n",
      "|    fps              | 690      |\n",
      "|    time_elapsed     | 119      |\n",
      "|    total_timesteps  | 82512    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000418 |\n",
      "|    n_updates        | 1716     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 523      |\n",
      "|    ep_rew_mean      | -36.9    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 180      |\n",
      "|    fps              | 691      |\n",
      "|    time_elapsed     | 123      |\n",
      "|    total_timesteps  | 85428    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000494 |\n",
      "|    n_updates        | 1777     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 526      |\n",
      "|    ep_rew_mean      | -36.8    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 184      |\n",
      "|    fps              | 691      |\n",
      "|    time_elapsed     | 125      |\n",
      "|    total_timesteps  | 86820    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000439 |\n",
      "|    n_updates        | 1806     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 537      |\n",
      "|    ep_rew_mean      | -35.6    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 188      |\n",
      "|    fps              | 691      |\n",
      "|    time_elapsed     | 130      |\n",
      "|    total_timesteps  | 90528    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0021   |\n",
      "|    n_updates        | 1883     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 536      |\n",
      "|    ep_rew_mean      | -35.9    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 192      |\n",
      "|    fps              | 692      |\n",
      "|    time_elapsed     | 135      |\n",
      "|    total_timesteps  | 93708    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000989 |\n",
      "|    n_updates        | 1950     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 550      |\n",
      "|    ep_rew_mean      | -34.2    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 196      |\n",
      "|    fps              | 692      |\n",
      "|    time_elapsed     | 139      |\n",
      "|    total_timesteps  | 96372    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000631 |\n",
      "|    n_updates        | 2005     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 567      |\n",
      "|    ep_rew_mean      | -28.9    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 200      |\n",
      "|    fps              | 692      |\n",
      "|    time_elapsed     | 142      |\n",
      "|    total_timesteps  | 98520    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000407 |\n",
      "|    n_updates        | 2050     |\n",
      "----------------------------------\n",
      "Mean reward: 676.0402049 +/- 1663.2218629026647\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "import os\n",
    "\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "#---------------------------------------------------------------------------\n",
    "env_id = \"VectorVelocity-v0\"\n",
    "#---------------------------------------------------------------------------\n",
    "seed =  42\n",
    "total_timesteps = 1e5\n",
    "n_train_env = os.cpu_count()\n",
    "#---------------------------------------------------------------------------\n",
    "model_save_dir = \"../models/dqn\"\n",
    "os.makedirs(model_save_dir, exist_ok=True)\n",
    "tensorboard_log_dir = \"../tensorboard_logs/dqn\"\n",
    "os.makedirs(tensorboard_log_dir, exist_ok=True)\n",
    "#---------------------------------------------------------------------------\n",
    "\n",
    "train_env = make_vec_env(env_id, n_envs=n_train_env, seed=seed, vec_env_cls=DummyVecEnv)\n",
    "\n",
    "model = DQN(\"MultiInputPolicy\", train_env, verbose=1, device=device, tensorboard_log=tensorboard_log_dir)\n",
    "model.learn(total_timesteps=total_timesteps)\n",
    "\n",
    "mean_reward_dqn, std_reward_dqn = evaluate_policy(model, model.get_env(), n_eval_episodes=10)\n",
    "print(f\"Mean reward: {mean_reward_dqn} +/- {std_reward_dqn}\")\n",
    "\n",
    "model.save(f\"{model_save_dir}/dqn_vector_velocity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![DQN Trainings History](tensorboard_logs\\screenshots\\dqn_trainings_history.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Because of the cleaner curve of the DQN we chose to do hyperparameter tuning on the DQN. The tuning can be found in dqn_hyperparameter_tuning.ipynb."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reinforcement_learning_vector_velocity",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
